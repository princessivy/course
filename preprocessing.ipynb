{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/princessivy/course/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDGdPU29MSVd"
      },
      "source": [
        "#Installs & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNbii3pkMOqZ"
      },
      "outputs": [],
      "source": [
        "!pip install ndjson --quiet\n",
        "!pip install beautifulsoup4 --quiet\n",
        "!pip install html2text --quiet\n",
        "!pip install nltk --quiet\n",
        "!pip install HanTa --quiet\n",
        "!pip install langdetect --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtG0Q4BBNlEk"
      },
      "outputs": [],
      "source": [
        "import ndjson\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import gzip\n",
        "from pathlib import Path\n",
        "# Uncomment the follwoing line if working in Google Colab \n",
        "# from google.colab import drive\n",
        "from collections import Counter, OrderedDict\n",
        "import html2text\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction import text\n",
        "from HanTa import HanoverTagger as ht\n",
        "from langdetect import detect\n",
        "import gc\n",
        "gc.enable()\n",
        "\n",
        "#for sentiment analysis\n",
        "from textblob import TextBlob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VyQ5CGxN1kZ"
      },
      "source": [
        "# Dataloader \n",
        "### Test-Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BBg5hWkNrOp",
        "outputId": "41cbb864-83d8-4141-ca22-c7c1c495db99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/gdrive')\n",
        "data_path = Path('/gdrive/MyDrive/industry_data/')\n",
        "file_name = 'test_small.ndjson.gz'\n",
        "\n",
        "with gzip.open(data_path/file_name, \"rt\", encoding='UTF-8') as file:\n",
        "    data = ndjson.load(file)\n",
        "df_test = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aEP79BCRujf"
      },
      "outputs": [],
      "source": [
        "   \n",
        "    # check for null entries\n",
        "if df_test.isnull().any(axis=None):\n",
        "    print('\\nPreview of data with null values:\\nxxxxxxxxxxxxx')\n",
        "    print(df_test[df_test.isnull().any(axis=1)].head(3))\n",
        "    #missingno.matrix(df_test)\n",
        "    #plt.show()\n",
        "else:\n",
        "  print('No null entries found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFf32zloN9eF",
        "outputId": "f85db1b3-0451-4215-bd35-cec0ee0281c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No null entries found\n"
          ]
        }
      ],
      "source": [
        "# check for null entries\n",
        "if df_test.isnull().any(axis=None):\n",
        "    print('\\nPreview of data with null values:\\nxxxxxxxxxxxxx')\n",
        "    print(df_test[df_test.isnull().any(axis=1)].head(3))\n",
        "    #missingno.matrix(df_test)\n",
        "    #plt.show()\n",
        "else:\n",
        "  print('No null entries found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AgvR25NOLtb",
        "outputId": "0fd0adaf-27e1-4173-ef50-83c09ac17e57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No duplicated entries found\n"
          ]
        }
      ],
      "source": [
        "# generate count statistics of duplicate entries\n",
        "if len(df_test[df_test.duplicated()]) > 0:\n",
        "    print('Number of duplicated entries: ', len(df_test[df_test.duplicated()]))\n",
        "    print(df_test[df_test.duplicated(keep=False)].sort_values(by=list(df_test.columns)).head())\n",
        "else:\n",
        "    print('No duplicated entries found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7WamG8dOmz8"
      },
      "source": [
        "### Train-Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSl9XlPHOeUY"
      },
      "outputs": [],
      "source": [
        "drive.mount('/gdrive')\n",
        "data_path = Path('/gdrive/MyDrive/industry_data/')\n",
        "file_name = 'train_small.ndjson.gz'\n",
        "with gzip.open(data_path/file_name, \"rt\", encoding='UTF-8') as file:\n",
        "    data = []\n",
        "    data = [ndjson.loads(line) for line in file]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBuF1gZ2Otv1"
      },
      "outputs": [],
      "source": [
        "# Nested List rausholen, Flat-List erzeugen, um Daten in DataFrame zu bekommen\n",
        "flat_list = [item for sublist in data for item in sublist]\n",
        "df_train = pd.DataFrame(flat_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy2lSaIQOvUE",
        "outputId": "2f2a2a3b-33ca-41ff-d80b-8a7dee1d7d14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No null entries found\n"
          ]
        }
      ],
      "source": [
        "# check for null entries\n",
        "if df_train.isnull().any(axis=None):\n",
        "    print('\\nPreview of data with null values:\\nxxxxxxxxxxxxx')\n",
        "    print(df_train[df_train.isnull().any(axis=1)].head(3))\n",
        "    #missingno.matrix(df_train)\n",
        "    #plt.show()\n",
        "else:\n",
        "  print('No null entries found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCLSMCcsOyKf"
      },
      "outputs": [],
      "source": [
        "# generate count statistics of duplicate entries\n",
        "if len(df_train[df_train.duplicated()]) > 0:\n",
        "    print('Number of duplicated entries: ', len(df_train[df_train.duplicated()]))\n",
        "    print(df_train[df_train.duplicated(keep=False)].sort_values(by=list(df_train.columns)).head())\n",
        "else:\n",
        "    print(\"No duplicated entries found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niQigmg2Ic7H"
      },
      "source": [
        "# \"Datasaver\" & \"data-reloader\"\n",
        "### To ndjson and ndjson.gz\n",
        "\n",
        "In order to save preprocessed data. If it is not saved row by row the run time crashes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4EYjrabQxPt"
      },
      "outputs": [],
      "source": [
        "# save to ndjson (either regular or gzip)\n",
        "\n",
        "def datasaver_to_zip(df, name):\n",
        "  # create flat list in dict form: {'col1': 'value', 'col2': 'value', ...} from df\n",
        "  flat_list_back = []\n",
        "  for i in range(len(df)):\n",
        "    line = df.loc[i].to_dict()\n",
        "    #line['industry'] = str(line['industry']) # use if idustry number (e.g. 13) should be enclosed in '' (e.g. '13')\n",
        "    flat_list_back.append([line])\n",
        "\n",
        "  filename_zip = str(name) + '.ndjson.gz'\n",
        "\n",
        "  with gzip.open(filename_zip, 'wt', encoding='UTF-8') as z:\n",
        "    for item in flat_list_back:\n",
        "      z.write('{}\\n'.format(ndjson.dumps(item)))\n",
        "\n",
        "def datasaver_to_ndjson(df, name):\n",
        "  # create flat list in dict form: {'col1': 'value', 'col2': 'value', ...} from df\n",
        "  flat_list_back = []\n",
        "  for i in range(len(df)):\n",
        "    line = df.loc[i].to_dict()\n",
        "    #line['industry'] = str(line['industry']) # use if idustry number (e.g. 13) should be enclosed in '' (e.g. '13')\n",
        "    flat_list_back.append([line])\n",
        "\n",
        "  filename = str(name) + '.ndjson'\n",
        "\n",
        "  # https://stackoverflow.com/questions/21058935/python-json-loads-shows-valueerror-extra-data\n",
        "  with open(filename, mode='w') as f:\n",
        "    for item in flat_list_back:\n",
        "      f.write('{}\\n'.format(ndjson.dumps(item))) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxvqvm8snxMH"
      },
      "outputs": [],
      "source": [
        "def data_reloader_from_zip(file_name):\n",
        "  with gzip.open(file_name, 'rt', encoding='UTF-8') as file:\n",
        "      data = []\n",
        "      data = [ndjson.loads(line.strip()) for line in file]\n",
        "\n",
        "  flat_list = [item for sublist in data for item in sublist]\n",
        "  df = pd.DataFrame(flat_list)\n",
        "\n",
        "  return df\n",
        "\n",
        "def data_reloader_from_ndjson(file_name):\n",
        "  with open(file_name, 'rt', encoding='UTF-8') as file:\n",
        "      data = []\n",
        "      data = [ndjson.loads(line.strip()) for line in file]\n",
        "\n",
        "  flat_list = [item for sublist in data for item in sublist]\n",
        "  df = pd.DataFrame(flat_list)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdG4GNwL3TbU",
        "outputId": "f11d6e03-9dab-4e3d-fa32-3a66abe0fc9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# save file to drive \n",
        "# make sure to have folder connected (use url for access and create link for your own drive)\n",
        "# Acess: https://drive.google.com/drive/folders/1qR-9z3uFmp5Nvsb_1QrR9lU8yNE8hi6l?usp=sharing\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "!cp test_html_to_text.ndjson.gz \"/gdrive/MyDrive/industry_data_processed/\" # exchange file name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fJp9_q7O1jz"
      },
      "source": [
        "# HTML Feature-Checkout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkeKfI3eSU9z"
      },
      "source": [
        "## Checking out Tag-Occurence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuujsgFLSX3Z"
      },
      "outputs": [],
      "source": [
        "all_tags = [] #der ersten 1000 Datensätze\n",
        "\n",
        "for i in range(1000): #len(df_train)\n",
        "  soup = BeautifulSoup(data[i][0]['html'], 'html.parser')\n",
        "  #for tag in soup.findAll(True):\n",
        "    #print(tag.name)\n",
        "  tags = set(tag.name for tag in BeautifulSoup(data[i][0]['html'], 'html.parser').find_all())\n",
        "  all_tags.extend(tags)\n",
        "\n",
        "  #print(soup.get_text()[:1024])\n",
        "  #print(tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkGrDfM8SZVW"
      },
      "outputs": [],
      "source": [
        "# count all text, print sorted by most occurences\n",
        "counted = Counter(all_tags)\n",
        "OrderedDict(counted.most_common())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJaJxFc-PXoE"
      },
      "source": [
        "## Get the whole Text between Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aolOjlJ9lO5I"
      },
      "outputs": [],
      "source": [
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfZlZg1odxu3"
      },
      "outputs": [],
      "source": [
        "def parse_to_text(html):\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "    # kill all script and style elements\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()    # rip it out\n",
        "\n",
        "    # get text\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # break into lines and remove leading and trailing space on each\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    # break multi-headlines into a line each\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    # drop blank lines\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBC8A3XhT7VR"
      },
      "outputs": [],
      "source": [
        "# eliminate html elements from text, return text elements\n",
        "# 21 minutes for train dataset\n",
        "\n",
        "# assign column for new text\n",
        "df_train = df_train.assign(html_to_text='')\n",
        "\n",
        "for line in range(0, len(df_train)):\n",
        "  content = parse_to_text(df_train.html[line])\n",
        "  df_train.html_to_text[line] = content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPs62mpFMNRH"
      },
      "outputs": [],
      "source": [
        "# on test dataset\n",
        "# 7 minutes\n",
        "\n",
        "df_test['html_to_text'] = ''\n",
        "\n",
        "for line in range(0, len(df_test)):\n",
        "  content = parse_to_text(df_test.html[line])\n",
        "  df_test.html_to_text[line] = content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI6GFzcQR33C"
      },
      "outputs": [],
      "source": [
        "# duplicate to keep working with original df (if necessary)\n",
        "df_test_html_to_text = df_test.copy()\n",
        "df_test_html_to_text=df_test_html_to_text.drop(columns='html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wsfPRppmMjy",
        "outputId": "a99d9014-e61e-4816-8641-953a42ce31f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "665"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLVaVqe4nVyh"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOwrX0A3nbn1",
        "outputId": "a43e9269-f21a-4005-ab09-37504cf88bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-7_XINontaF"
      },
      "outputs": [],
      "source": [
        "# https://medium.com/analytics-vidhya/applying-text-classification-using-logistic-regression-a-comparison-between-bow-and-tf-idf-1f1ed1b83640\n",
        "\n",
        "# hier könnten wir sprachenabhängig arbeiten: clean_text_german(), clean_text_english() und anhand des lang-tags anwenden\n",
        " \n",
        "def clean_text(mixed_text):\n",
        "    '''Text Preprocessing '''\n",
        "    \n",
        "    # convert words to lower case\n",
        "    content = mixed_text.lower()\n",
        "    \n",
        "    # ENGLISH use this for english text\n",
        "    # Expand contractions (you've -> you have)\n",
        "    #if True:\n",
        "    #    text = text.split()\n",
        "    #    new_text = []\n",
        "    #    for word in text:\n",
        "    #        if word in contractions:\n",
        "    #            new_text.append(contractions[word])\n",
        "    #        else:\n",
        "    #            new_text.append(word)\n",
        "    #    text = \" \".join(new_text)\n",
        "    \n",
        "    # Format words and remove unwanted characters\n",
        "    #content = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', content, flags=re.MULTILINE) # brauchen wir nicht mehr, da schon geparst\n",
        "    #content = re.sub(r'\\<a href', ' ', content)\n",
        "    content = re.sub(r'&amp;', '', content) \n",
        "    content = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', content)\n",
        "    content = re.sub(r'<br />', ' ', content)\n",
        "    content = re.sub(r'\\'', ' ', content)\n",
        "    \n",
        "    # remove stopwords\n",
        "    content = content.split()\n",
        "    stops = set(stopwords.words('german'))\n",
        "    content = [w for w in content if not w in stops]\n",
        "    content = ' '.join(content)\n",
        "\n",
        "    # tokenize each word\n",
        "    content =  nltk.WordPunctTokenizer().tokenize(content)\n",
        "    \n",
        "    # lemmatize each token in German (reduce words to stem)\n",
        "    tagger = ht.HanoverTagger('morphmodel_ger.pgz')\n",
        "    word_list = []\n",
        "    for w in content:\n",
        "        lemma = [lemma for (word,lemma,pos) in tagger.tag_sent(w.split())]\n",
        "        word_list.append(' '.join(lemma))\n",
        "\n",
        "    # ENGLISH use this for english text\n",
        "    # lemmatize each token\n",
        "    #lemm = nltk.stem.WordNetLemmatizer()\n",
        "    #content = list(map(lambda word:list(map(lemm.lemmatize, word)), content))\n",
        "    \n",
        "    return word_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5M1S-lUlFBn"
      },
      "outputs": [],
      "source": [
        "df_test_html_to_text = data_reloader_from_zip('test_html_to_text.ndjson.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvpOp02Nn4Hu"
      },
      "outputs": [],
      "source": [
        "df_test_html_to_text['html_cleaned'] = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssB5jBYIlapF",
        "outputId": "c453bc3c-b2f7-4ee7-a492-b05c334c940a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8396"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_test_html_to_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkROPhTlokvp"
      },
      "outputs": [],
      "source": [
        "for line in range(4000):\n",
        "    content = clean_text(df_test_html_to_text['html_to_text'][line])\n",
        "    print(line) #debugging\n",
        "    df_test_html_to_text.html_cleaned[line] = content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUJS9LuCZ5lP"
      },
      "outputs": [],
      "source": [
        "datasaver_to_zip(df_test_html_to_text, 'df_test_cleaned_text')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z-1M2WBPgVY"
      },
      "source": [
        "## Specific Tags for additional Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW7i0lYFPPtA"
      },
      "outputs": [],
      "source": [
        "def getHTML(url):\n",
        "    # später anpassen, wenn wir live-url abfragen! evtl. Fallunterscheidung?!\n",
        "    '''    r = requests.get(url)\n",
        "    r.text'''\n",
        "    return  BeautifulSoup(url, 'html.parser')\n",
        "\n",
        "\n",
        "\n",
        "## Img-Description from IMG-Tag\n",
        "def getImgDescriptionHTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    results = soup.find_all('img', alt = True)\n",
        "    img_description = []\n",
        "    for x in range(0,len(results)):\n",
        "      first_result = results[x]\n",
        "      img_description.append(first_result['alt'])\n",
        "    \n",
        "    return list(filter(None, img_description))\n",
        "\n",
        "\n",
        "## Title\n",
        "def getTitleHTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    if (soup.title is not None):\n",
        "        return str(soup.title.string)\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "## h1\n",
        "def getH1HTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    heading = soup.findAll('h1')\n",
        "    n = len(heading)\n",
        "\n",
        "    liste = []\n",
        "    for x in range(n):\n",
        "      liste.append(str.strip(heading[x].text))\n",
        "\n",
        "    return list(filter(None, liste))\n",
        "\n",
        "\n",
        "## h2\n",
        "def getH2HTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    heading = soup.findAll('h2')\n",
        "    n = len(heading)\n",
        "\n",
        "    liste = []\n",
        "    for x in range(n):\n",
        "      liste.append(str.strip(heading[x].text))\n",
        "\n",
        "    return list(filter(None, liste))\n",
        "\n",
        "\n",
        "## h3\n",
        "def getH3HTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    heading = soup.findAll('h3')\n",
        "    n = len(heading)\n",
        "\n",
        "    liste = []\n",
        "    for x in range(n):\n",
        "      liste.append(str.strip(heading[x].text))\n",
        "\n",
        "    return list(filter(None, liste))\n",
        "\n",
        "\n",
        "## strong - fragwürdig\n",
        "def getStrongHTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    heading = soup.findAll('strong')\n",
        "    n = len(heading)\n",
        "\n",
        "    liste = []\n",
        "    for x in range(n):\n",
        "      liste.append(str.strip(heading[x].text))\n",
        "\n",
        "    return list(filter(None, liste))\n",
        "\n",
        "\n",
        "## bold\n",
        "def getBoldHTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    heading = soup.findAll('bold')\n",
        "    n = len(heading)\n",
        "\n",
        "    liste = []\n",
        "    for x in range(n):\n",
        "      liste.append(str.strip(heading[x].text))\n",
        "\n",
        "    return list(filter(None, liste))\n",
        "  \n",
        "\n",
        "## language code\n",
        "def getLangHTMLtag(url):\n",
        "    \n",
        "    try:\n",
        "      soup = getHTML(url)\n",
        "      body_text = soup.body.get_text()\n",
        "      return detect(body_text)\n",
        "    \n",
        "    except:\n",
        "      return str(\"NaN\")\n",
        "    \n",
        "    \n",
        "## figcaption\n",
        "def getFigCaptionHTMLtag(url):\n",
        "    soup = getHTML(url)\n",
        "\n",
        "    heading = soup.findAll('figcaption')\n",
        "    n = len(heading)\n",
        "\n",
        "    liste = []\n",
        "    for x in range(n):\n",
        "      liste.append(str.strip(heading[x].text))\n",
        "\n",
        "    return list(filter(None, liste))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-j0cPot2Cdy"
      },
      "outputs": [],
      "source": [
        "df_test = df_test.assign(img_alt='', title='', h1='', h2='', h3='', strong='', bold='', lang_code='', figcaption='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Av3H3Gp52ug"
      },
      "outputs": [],
      "source": [
        "# Befüllen der Extra-Features\n",
        "\n",
        "def retrieve_features(df):\n",
        "    for i in range (4, 13):\n",
        "        for j in range(0, len(df)):\n",
        "            if i == 4:\n",
        "              df.iloc[:, i][j] = getImgDescriptionHTMLtag(df.html[j])\n",
        "            elif i == 5:\n",
        "              df.iloc[:, i][j] = getTitleHTMLtag(df.html[j])\n",
        "            elif i == 6:\n",
        "              df.iloc[:, i][j] = getH1HTMLtag(df.html[j])\n",
        "            elif i == 7:\n",
        "              df.iloc[:, i][j] = getH2HTMLtag(df.html[j])\n",
        "            elif i == 8:\n",
        "              df.iloc[:, i][j] = getH3HTMLtag(df.html[j])\n",
        "            elif i == 9:\n",
        "              df.iloc[:, i][j] = getStrongHTMLtag(df.html[j])\n",
        "            elif i == 10:\n",
        "              df.iloc[:, i][j] = getBoldHTMLtag(df.html[j])\n",
        "            elif i == 11:\n",
        "              df.iloc[:, i][j] = getLangHTMLtag(df.html[j])\n",
        "            elif i == 12:\n",
        "              df.iloc[:, i][j] = getFigCaptionHTMLtag(df.html[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15PG8NnnO2oT"
      },
      "outputs": [],
      "source": [
        "# aus List-Elementen in DataFrame einfache Strings machen, um besser cleanen zu können\n",
        "def convert_features_toString(df):\n",
        "    # img\n",
        "    for x in range(len(df)):\n",
        "        df.img_alt[x] = ' '.join(df.img_alt[x])\n",
        "\n",
        "    # h1\n",
        "    for x in range(len(df)):\n",
        "        df.h1[x] = ' '.join(df.h1[x])\n",
        "\n",
        "    # h2\n",
        "    for x in range(len(df)):\n",
        "        df.h2[x] = ' '.join(df.h2[x])\n",
        "\n",
        "    # h3\n",
        "    for x in range(len(df)):\n",
        "        df.h3[x] = ' '.join(df.h3[x])\n",
        "\n",
        "    # strong\n",
        "    for x in range(len(df)):\n",
        "        df.strong[x] = ' '.join(df.strong[x])\n",
        "\n",
        "    # bold\n",
        "    for x in range(len(df)):\n",
        "        df.bold[x] = ' '.join(df.bold[x])\n",
        "\n",
        "    # figcaption\n",
        "    for x in range(len(df)):\n",
        "        df.figcaption[x] = ' '.join(df.figcaption[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr6pYEXPRwDF"
      },
      "outputs": [],
      "source": [
        "# alle extra-features durchgehen um zu gucken, welche wir noch durch clean_text laufen lassen müssen\n",
        "columns = df_train_reloaded.columns.tolist()\n",
        "columns = columns[4:13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EoUonawEhml"
      },
      "outputs": [],
      "source": [
        "# lassen lang-code weg\n",
        "columns = ['img_alt','title','h1','h2','h3','strong','bold','figcaption']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUnqbMXzSM4D"
      },
      "outputs": [],
      "source": [
        "# alle extra-features durch clean_text schicken, ohne lang-feature\n",
        "def clean_dataframe(df):\n",
        "    columns = ['img_alt','title','h1','h2','h3','strong','bold','figcaption']\n",
        "    for x in columns:\n",
        "        print(x)\n",
        "        for y in range(len(df)):\n",
        "            df[x][y] = clean_text(df[x][y])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "KA9IDj5tRukC",
        "outputId": "f681f2b5-480b-4f45-f877-11cc7a4ed4a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/0w/wzymnpfd4rjdkg4k8khhk6dc0000gn/T/ipykernel_44039/2078399981.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.iloc[:, i][j] = getImgDescriptionHTMLtag(df.html[j])\n",
            "/var/folders/0w/wzymnpfd4rjdkg4k8khhk6dc0000gn/T/ipykernel_44039/2078399981.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.iloc[:, i][j] = getTitleHTMLtag(df.html[j])\n",
            "/var/folders/0w/wzymnpfd4rjdkg4k8khhk6dc0000gn/T/ipykernel_44039/2078399981.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.iloc[:, i][j] = getH1HTMLtag(df.html[j])\n",
            "/var/folders/0w/wzymnpfd4rjdkg4k8khhk6dc0000gn/T/ipykernel_44039/2078399981.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.iloc[:, i][j] = getH2HTMLtag(df.html[j])\n",
            "/var/folders/0w/wzymnpfd4rjdkg4k8khhk6dc0000gn/T/ipykernel_44039/2078399981.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.iloc[:, i][j] = getH3HTMLtag(df.html[j])\n",
            "/var/folders/0w/wzymnpfd4rjdkg4k8khhk6dc0000gn/T/ipykernel_44039/2078399981.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.iloc[:, i][j] = getStrongHTMLtag(df.html[j])\n",
            "/var/folders/0w/wzymnpfd4rjdkg4k8khhk6dc0000gn/T/ipykernel_44039/2078399981.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.iloc[:, i][j] = getBoldHTMLtag(df.html[j])\n",
            "/var/folders/0w/wzymnpfd4rjdkg4k8khhk6dc0000gn/T/ipykernel_44039/2078399981.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.iloc[:, i][j] = getLangHTMLtag(df.html[j])\n",
            "/var/folders/0w/wzymnpfd4rjdkg4k8khhk6dc0000gn/T/ipykernel_44039/2078399981.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.iloc[:, i][j] = getFigCaptionHTMLtag(df.html[j])\n",
            "/var/folders/0w/wzymnpfd4rjdkg4k8khhk6dc0000gn/T/ipykernel_44039/41399377.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.img_alt[x] = ' '.join(df.img_alt[x])\n",
            "/var/folders/0w/wzymnpfd4rjdkg4k8khhk6dc0000gn/T/ipykernel_44039/41399377.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.bold[x] = ' '.join(df.bold[x])\n",
            "/var/folders/0w/wzymnpfd4rjdkg4k8khhk6dc0000gn/T/ipykernel_44039/41399377.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.figcaption[x] = ' '.join(df.figcaption[x])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "img_alt\n",
            "title\n",
            "h1\n",
            "h2\n",
            "h3\n",
            "strong\n",
            "bold\n",
            "figcaption\n"
          ]
        }
      ],
      "source": [
        "# perform preprocessing\n",
        "df_test = df_test.assign(img_alt='', title='', h1='', h2='', h3='', strong='', bold='', lang_code='', figcaption='')\n",
        "retrieve_features(df_test)\n",
        "convert_features_toString(df_test)\n",
        "clean_dataframe(df_test)\n",
        "df_test = df_test.drop(columns=['html'], axis=1)\n",
        "datasaver_to_ndjson(df=df_test, name='df_test_preprocessed')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyk0NWAyY0wL"
      },
      "source": [
        "# Datei(en) exportieren"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJoZxxx_Y9QE"
      },
      "outputs": [],
      "source": [
        "df_train_work.to_json('train_preprocessed.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment-Analysis"
      ],
      "metadata": {
        "id": "2Gc73sjeR6ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['sentiment_analysis'] =''"
      ],
      "metadata": {
        "id": "ablJIBJzStU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spalte aus welcher sentiment-analysis gemacht wird, in string casten\n",
        "df_train['pure_text'] = df_train['pure_text'].astype(str)"
      ],
      "metadata": {
        "id": "La4lQLpxSYh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(0, len(df_train)):\n",
        "  df_train.sentiment_analysis[x] = round(TextBlob(df_train['pure_text'][x]).sentiment.polarity,2)"
      ],
      "metadata": {
        "id": "LJRt5fTgR7su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Überprüfung des means\n",
        "df_train.groupby('industry_label')['sentiment_analysis'].mean()"
      ],
      "metadata": {
        "id": "9GmiuMafXDM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Preprocessing Capstone.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}